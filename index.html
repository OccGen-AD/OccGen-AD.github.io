<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
  .center-div {
    display: flex;
    justify-content: center;
  }
</style>


  <title>OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wgqtmac.github.io/ target="_blank">Guoqing Wang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://zhongdao.github.io/" target="_blank">Zhongdao Wang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=au87GKsAAAAJ&hl=en" target="_blank">Pin Tang</a><sup>1</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://OccGen-AD.github.io" target="_blank">Jilai Zheng</a><sup>1</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://OccGen-AD.github.io" target="_blank">Xiangxuan Ren</a><sup>1</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://OccGen-AD.github.io" target="_blank">Bailan Feng</a><sup>2</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://vision.sjtu.edu.cn/" target="_blank">Chao Ma</a><sup>1</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                      <span class="author-block"><sup>1</sup><a href="https://www.sjtu.edu.cn/"
                              target="_blank">Shanghai Jiao Tong University</a>,</span>
                      <span class="author-block"><sup>2</sup><a href="https://www.ucmerced.edu/"
                              target="_blank">Huawei Noah's Ark Lab</a></span>
                  </div>

            

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                      <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(Coming Soon)</span>
                  </a>
                </span>
                      
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="videos/video.mp4"
        type="video/mp4">
      </video>
      
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing solutions for 3D semantic occupancy prediction typically treat the task as a one-shot 
            3D voxel-wise segmentation perception problem. These discriminative methods focus on learning 
            the mapping between the inputs and occupancy map in a single step, lacking the ability to 
            gradually refine the occupancy map and the reasonable scene imaginative capacity to complete 
            the local regions somewhere. In this paper, we introduce OccGen, a simple yet powerful generative
            perception model for the task of 3D semantic occupancy prediction. OccGen adopts a 
            ``noise-to-occupancy'' generative paradigm, progressively inferring and refining the occupancy map
            by predicting and eliminating noise originating from a random Gaussian distribution. OccGen consists
            of two main components: a conditional encoder that is capable of processing multi-modal inputs, and 
            a progressive refinement decoder that applies diffusion denoising using the multi-modal features as 
            conditions.  A key insight of this generative pipeline is that the diffusion denoising process is 
            naturally able to model the coarse-to-fine refinement of the dense 3D occupancy map, therefore producing
            more detailed predictions. Extensive experiments on several occupancy benchmarks demonstrate the effectiveness
            of the proposed method compared to the state-of-the-art methods. For instance,  OccGen relatively enhances the 
            mIoU by  9.5%, 6.3%, and 13.3% on nuScenes-Occupancy dataset under the muli-modal, LiDAR-only, and camera-only 
            settings, respectively. Moreover, as a generative perception model, OccGen exhibits desirable properties that
            discriminative models cannot achieve, such as providing uncertainty estimates alongside its multiple-step predictions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-">
                        <h1 class="title is-3">Introduction</h2>
                        <div class="content has-text-justified">



                            <div>
                                <td colspan="3"><img src="pics/Introduction.png" alt="" width="1000" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                    (a) The generative diagram of semantic segmentation (seg.), object detection (det.), and 3D semantic occupancy prediction (occ.).
   (b) Compared to previous discriminative methods with a single forward evaluation scheme, our OccGen is a generative model that can generate occupancy map in a coarse-to-fine manner.
                                </p>
                                </p>
                            </div>
                        </div>
                    </div>
    </section>

   <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-">
                        <h2 class="title is-3">Method</h2>
                        <div class="content has-text-justified">



                            <div>
                                <td colspan="3"><img src="pics/Overview.png" alt="" width="1000" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  The proposed OccGen framework. It has an encoder-decoder structure. The conditional encoder extracts the features 
                                  from the inputs as the condition. The progressive refinement decoder consists of a stack of refinement layers and 
                                  an occupancy head, which takes the 3D noise map, sampling step, and conditional multi-scale fusion features as 
                                  inputs and progressively generates the occupancy prediction.
                                </p>
                                </p>
                            </div>
                        </div>
                    </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-">
                        <h2 class="title is-3">Conditional Encoder</h2>
                        <div class="content has-text-justified">



                            <div class="center-div">
                                <td colspan="2"><img src="pics/encoder.png" alt="" width="800" height="250" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  The conditional encoder consists of three main components: a multi-modal encoder, a fusion module,
                                  and an occupancy backbone. As shown in the above figure, the multi-modal encoder is a two-stream 
                                  structure, comprising of LiDAR and camera streams. 
                                </p>
                            </div>
                        </div>
                    </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-">
                        <h2 class="title is-3">Progressive Refinement Decoder</h2>
                        <div class="content has-text-justified">



                            <div class="center-div">
                                <td colspan="2"><img src="pics/decoder.png" alt="" width="800" height="250" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  The progressive refinement decoder of OccGen consists of a stack of refinement
                                  layers and an occupancy head. As illustrated in the above figure, the refinement
                                  layer takes as input the random noise map or the predicted noise map from the 
                                  last step, the current sampling step $t$, and the multi-scale fusion features. 
                                  The refinement layer utilizes efficient 3D deformable cross-attention and 
                                  self-attention to refine the 3D Gaussian noise map.
                                </p>
                            </div>
                        </div>
                    </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-">
                        <h2 class="title is-3">Experimental Results</h2>
                        <div class="content has-text-justified">



                            <div>
                                <td colspan="2"><img src="pics/exp_nusc.png" alt="" width="1000" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  Semantic occupancy prediction results on nuScenes-Occupancy validation set. The C,D,L,M denotes camera, depth, LiDAR and multi-modal. 
                                  Best camera-only, LiDAR-only, and multi-modal results are marked red, blue, and black, respectively. Observations show that OccGen 
                                  outperforms all existing competitors, regardless of whether the camera-only, LiDAR-only, or multi-modal methods, which demonstrates 
                                  the effectiveness of OccGen for semantic occupancy prediction.
                                </p>
                            </div>

                          <div>
                                <td colspan="2"><img src="pics/exp_kitti.png" alt="" width="1000" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  Semantic Scene Completion results on SemanticKITTI validation set.} &dagger denotes the results provided by MonoScene. The results show
                                  that OccGen achieves the highest mIoU compared with all existing competitors.
                                </p>
                            </div>

                          <div>
                                <td colspan="2"><img src="pics/vis.png" alt="" width="1000" /></td>
                            </div>
                            <div class="content has-text-justified">
                                <p>
                                  Qualitative results of the 3D semantic occupancy predictions on nuScenes-Occupancy. The leftmost column shows the input surrounding images,
                                  and the following four columns visualize the 3D semantic occupancy results from the ground truth, CONet, OccGen(step1), and OccGen(step2).
                                  The regions highlighted by rectangles indicate that these areas have obvious differences (better viewed when zoomed in). It is obvious that 
                                  the regions of ``drivable surface'' and ``sidewalk'' predicted by our OccGen have higher continuity and integrity, and can effectively reduce
                                  a large number of hole areas compared with CONet.
                                </p>
                            </div>
                          
                        </div>
                    </div>
    </section>

              



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
